\section{Portal RPC}

Portal RPC provides basic mechanisms for

\begin{itemize}

  \item sending requests through imports and receiving replies,

  \item receiving and processing requests through exports and sending replies,

  \item performing bulk data transfer, and

  \item error recovery.

\end{itemize}

\subsection{Client Side Interface}

We will first discuss the interface of Portal RPC without going into
implementation details. We will use the LDLM's send mechanism as an example.
For this example, LDLM is sending a blocking AST RPC to a client that is
the owner of a given lock (\url{ldlm_server_blocking_ast}) and also acting as
the lock manager. This example will help us better illustrate how a client uses
Portal RPC API.  

First, we prepare the size for this request as given below.

\begin{Verbatim}
struct ldlm_request *req;
__u32 size[] = { [MSG_PTLRPC_BODY_OFF] = sizeof(struct ptlrpc_body),
    [DLM_LOCKREQ_OFF] = sizeof (*body) };
\end{Verbatim}

A request can be seen as a sequence of records, where the first record has an
offset of 0, the second record has an offset value 1, and so on. Once the size
is determined, we can invoke \url{ptlrpc_prep_req()}. The prototype of this
function is as follows:

\begin{Verbatim}
struct ptlrpc_request *
  ptlrpc_prep_req(struct obd_import *imp, __u32 version, int opcode, 
                int count, __u32 *lengths, char **bufs)
\end{Verbatim}

The RPC communication needs an import on the client's side, and it is created
during the connect phase. The \url{*imp} points to this particular import, and
\url{version} specifies the version to be used by the Portal RPC internals to
pack the data. Packing here is the  on-the-wire format that defines how the
buffers are actually situated in the network packet. Each subsystem defines a
version number to distinguish its layout request---for example, MDC and MDS
define their version, MGC and MGS define another. 

The \url{opcode} specifies what this request is about. Each subsystem defines
a set of operation codes (see \url{lustre_idl.h} for more information).
The \url{count} is the number of buffers needed for this request, and
\url{*length} is an array with each element specifying the size of the
corresponding requested buffer.  The final parameter signals Portal RPC to
accept (copy the data over) and process the incoming packet as is.  For our
example, this function is invoked as given below.

\begin{Verbatim}
req = ptlrpc_prep_req(lock->l_export->exp_imp_reverse,
    LUSTRE_DLM_VERSION, LDLM_BL_CALLBACK, 2, size, NULL);
\end{Verbatim}

This declaration indicates that there are two buffers requested and the size of
each buffer is represented by \url{size} in the parameter list.

In addition to the housekeeping, the above call allocates the request buffer and
saves it in  \url{req->rq_reqmsg}.  The address of the interested record can be
acquired by specifying the record offset:

\begin{Verbatim}
body = lustre_msg_buf(req->rq_reqmsg, DLM_LOCKREQ_OFF, sizeof(*body)); 
\end{Verbatim}

On the server side, we see this same helper method with similar input parameters
used to extract the field of interest. Once the buffer structure is obtained,
necessary fields for the request can be further filled in. After all is done,
there are several ways to send the request, as listed here.

\begin{description}

\item[\url{ptl_send_rpc()}] A primitive form of RPC sending, which doesn't wait
for a reply and doesn't try to resend when failure occurs. It is not a preferred
way of doing RPC send.

\item[\url{ptlrpcd_add_req()}] A completely asynchronous RPC sending, handled
by the ptlrpcd daemon.

\item[\url{ptlrpc_set_wait()}] A synchronous RPC sending of multiple messages,
which returns only when it gets if all of the replies back. First,
\url{ptlrpc_set_add_req()} must be used to add the request to a pre-initialized
set, which contains one or more requests that must be sent together. 

\item[\url{ptlrpc_queue_wait()}] Probably the most common way of sending RPC,
which is synchronous and returns only after an RPC request is sent and a reply is
received.

\end{description}

The last step after invoking this RPC request is to release the resource
references by calling \url{ptlrpc_req_finished(req)}.

\subsection{Server Side Interface}

The server side uses Portal RPC in a completly different way from the
client side. First, it initializes the service with the function call given
below.

\begin{Verbatim}
struct ptlrpc_service * ptlrpc_init_svc (
    int nbufs,             /* num of buffers to allocate */
    int bufsize,           /* size of above requested buffer */ 
    int max_req_size,      /* max request size server will accept */
    int max_reply_size,    /* max reply size server will send */ 
    int req_portal,        /* req service port in lnet */
    int rep_portal,        /* rep service port in lnet */
    int watchdog_factor,   /* wait time for handler to finish */
    svc_handler_t handler, /* service handler function */
    char *name,            /* service name */
    cfs_proc_dir_entry_t *proc_entry, /* for procfs */
    svcreq_printfn_t svcreq_printfn,  /* for procfs */
    int min_threads,        /* min # of threads to start */
    int max_threads,        /* max # of threads to start */
    char *threadname        /* thread name prefix */
)
\end{Verbatim}

Once the call returns, the request can come in and the registered handler
function will be called. Usually, server divides the task at hand into several
types. For each type, it creates a different pool of threads. These threads
could share the same handler. The reason for different pools is to prevent
starvation. In some cases, multiple pools also prevent deadlocks, where all
threads are waiting for some resource to become free to handle a new RPC. 

\subsection{Bulk Transfer}
\label{sec:bulktransfer}

The client first sends a bulk RPC request. Let's assume this is a write request.
It contains descriptions of what to send. Now the server processes the request,
allocates the space, and then takes control of the data transfer. The next
RPC from the server will perform a bulk transfer of the data to the
pre-allocated spaces. One such example is done in the
\url{osc_brw_pre_request()}. Let's walk through the process: 

\begin{enumerate}

\item Bulk transfer is initiated with the preparation as discussed before.
However, the prepare request is different in the sense that we are asking the
request from a pre-allocated pool, which is the case if the request itself can be
associated with a low-memory situation.

\begin{Verbatim}
req = ptlrpc_pre_req_pool(cli->cl_import, LUSTRE_OST_VERSION, 
                                  opc, 4, size, null, pool)
\end{Verbatim}

The \url{opc} in this case can be, for example, \url{OST_WRITE}.

\item Next, we specify the service portal. In the import structure, there is a
default portal that this request will go, but in this case for the sake of the
example, let's assume the request will be handled by a specific portal:

\begin{Verbatim}
req->rq_request_portal = OST_IO_PORTAL;
\end{Verbatim}

\item Then, we need to prepare the bulk request. We pass in as parameters the
pointer to the request, number of pages, type, and destination portal. The
return is a bulk descriptor for this request. Notice that the bulk request goes
to a different portal:

\begin{Verbatim}
struct ptlrpc_bulk_desc desc = ptlrpc_prep_bulk_imp(req, page_count, 
             BULK_GET_SOURCE, OST_BULK_PORTAL);
\end{Verbatim}

\item For each page to be transferred, we invoke the
\url{ptlrpc_prep_bulk_page()} and add one page at time to the bulk descriptor.
There is a flag in the request indicating that this is a bulk request and we
should check this descriptor to obtain all the layout information on pages.

\begin{Verbatim}
struct ptlrpc_request {
    ...
    struct ptlrpc_bulk_desc *rq_bulk;
    ...
}
\end{Verbatim}

\end{enumerate}

At the server side, the overall preparation structure is similar, but instead
of preparing for import, now it prepares for an export. An example of it can be
seen in \url{ost_brw_read()} in ost handler.

\begin{Verbatim}
desc = ptlrpc_prep_bulk_exp(req, npages, BULK_PUT_SOURCE, OST_BULK_PORTAL);
\end{Verbatim}

The server side also needs to prepare each bulk page. Later, the server side
can start the transfer by:

\begin{Verbatim}
rc = ptlrpc_start_bulk_transfer(desc);
\end{Verbatim}

At this point in time, the first RPC request from the client has been
processed by the server, and the server is ready for the bulk data transfer.
Now the server can start the bulk RPC transfer as we mentioned at the
begining of this section.

\subsubsection*{NRS Optimization}

On the server side, another point to note is that we can receive a huge number
of descriptors that describe the page layout to read or write. This presents an
opportunity for an optimization if there are any neighboring reads or writes
going to the same region. If there are, perhaps they can be grouped and
processed together. That is the subject of the Network Request Scheduler
(NRS). This also displays the significance of a two-phase bulk transfer, which
allows us to get an idea of the incoming/outgoing data without actually getting the
data first, so that they can be reorganized for better performance. The second
reason for the two-phase operation is that as the service initialization
increases on the server, a certain amount of buffer space is allocated. When
client requests come in, they will be buffered in this space first before further
processing, as pre-allocating a huge amount of space there just to accommodate
potential bulk transfers is not preferred. Also, it is important not to overwhelm
server buffer space with big data chunks, and two-phase operation helps in that
context as well.

\subsection{Error Recovery: A Client Perspective}

Most of the recovery mechanism is implemented at the Portal RPC layer.  We
start with a portal PRC request which is passed down from the upper level.
Inside Portal RPC, there are two lists maintained by the import that are
important to our discussion. These are the \textit{sending} and \textit{replay}
lists. The import also maintains \url{imp->states} and \url{imp->flags}. The
possible states are \textit{full}, \textit{connecting}, \textit{disconnecting},
and \textit{recovery}, and flags can be \textit{invalid}, \textit{active} and
\textit{inactive}. 

After the health status of the import is checked, the send request will
continue. The sequence of the steps is outlined here:

\begin{enumerate}

\item Send the RPC request, then save it into the sending list. Also start
the \textbf{obd timer} at the client side.

\item If a server reply is received before the timeout expires and the request is
\textit{replayable}\footnote{Replay request only refers to those that will
modify the on-disk data. For example, read is not a replayable request, but
during recovery, they can still be resent if they are in the sending list.},
then unlink it from the sending list and link it to the replay list. If the
request is not replayable, then upon receiving the reply, remove the request
from the sending list.

A reply from the server doesn't necessarily mean it committed the data to the
disk (assuming the request alters on-disk data).  Therefore, we must wait for a
transaction commit (a transaction num) from the server, which means that the
change is now safely committed to the disk.  This last server-committed
transaction number is usually piggybacked with every server reply.  

Usually, a request from MDC to MDS is replayable, but an OSC to OST request is
not, and this is only true if the asynchronous journal update is not enabled.
There are two reasons:

  \begin{itemize}
  
  \item First, a data request (read or write) from OSC to OST can be very
  large, and keeping them in memory for replay can be a huge burden
  on memory.

  \item Second, OST uses only direct I/O (at least for now). The reply itself,
  along with transaction number, is enough of a guarantee to say the commit is
  done.

  \end{itemize}

\item If a timer expires, then client marks this import state from \textit{full}
to \textit{disconnect}. Now the pinger kicks in and if the server responds to
the pinger, then the client will try to reconnect (connect with reconnect
flags). 

\item If the reconnect is successful, then we start the recovery process. We
now mark the state as \textit{recovery} and start sending the requests in the
replay list first, followed by the requests in the sending list.

\end{enumerate}

The key point about the pinger is that if requests are being sent frequently enough, 
then the pinger is not needed. It is activated only if a client 
has an extended idle period, and the pinger is used to keep the
connection alive with the server so that it will not get evicted due to
inactivity. On the other hand, if client went offline for whatever reason,
the server will not get pinged by the client, and the server can still evict this
client.

